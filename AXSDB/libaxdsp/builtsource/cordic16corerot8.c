#include "libaxdsp.h"

#define cordic16_core_rot cordic16_core_rot8

#if defined(SDCC)

// dec2hex(round(arg(1+i*2.^-(0:15))/2/pi*2^16))
// 0x2000, 0x12E4, 0x09FB, 0x0511, 0x028B, 0x0146, 0x00A3, 0x0051
// 0x0029, 0x0014, 0x000A, 0x0005, 0x0003, 0x0001, 0x0001, 0x0000

__reentrantb uint16_t cordic16_core_rot(uint16_t phase) __reentrant
{
	// special calling conventions!
	// R3:R2: I
	// R5:R4: Q
	// DPTR: phase
	__asm
	;; unrolled
	;; pre stage
	mov	a,dph
	rl	a
	xrl	a,dph
	jnb	acc.7,00000$
	clr	c
	clr	a
	subb	a,r2
	mov	r2,a
	clr	a
	subb	a,r3
	mov	r3,a
	clr	c
	clr	a
	subb	a,r4
	mov	r4,a
	clr	a
	subb	a,r5
	mov	r5,a
	mov	a,dph
	xrl	a,#0x80
	mov	dph,a
00000$:
	__endasm;
	__asm
	;; stage 0
	mov	a,dph
	jb	acc.7,00004$
	clr	c
	mov	a,r2
	mov	r0,a
	subb	a,r4
	mov	r2,a
	mov	a,r3
	mov	r1,a
	subb	a,r5
	mov	r3,a
	mov	a,r4
	add	a,r0
	mov	r4,a
	mov	a,r5
	addc	a,r1
	mov	r5,a
	mov	a,dph
	add	a,#(-0x2000)>>8
	mov	dph,a
	sjmp	00005$
00004$:	mov	a,r2
	mov	r0,a
	add	a,r4
	mov	r2,a
	mov	a,r3
	mov	r1,a
	addc	a,r5
	mov	r3,a
	clr	c
	mov	a,r4
	subb	a,r0
	mov	r4,a
	mov	a,r5
	subb	a,r1
	mov	r5,a
	mov	a,dph
	add	a,#0x2000>>8
	mov	dph,a
00005$:
	__endasm;
	__asm
	;; stage 1
	mov	a,r3
	mov	c,acc.7
	rrc	a
	mov	r1,a
	mov	a,r2
	rrc	a
	mov	r0,a
	mov	a,r5
	mov	c,acc.7
	rrc	a
	mov	r7,a
	mov	a,r4
	rrc	a
	mov	r6,a
	mov	a,dph
	jb	acc.7,00008$
	clr	c
	mov	a,r2
	subb	a,r6
	mov	r2,a
	mov	a,r3
	subb	a,r7
	mov	r3,a
	mov	a,r4
	add	a,r0
	mov	r4,a
	mov	a,r5
	addc	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#-0x12E4
	mov	dpl,a
	mov	a,dph
	addc	a,#(-0x12E4)>>8
	mov	dph,a
	sjmp	00009$
00008$:	mov	a,r2
	add	a,r6
	mov	r2,a
	mov	a,r3
	addc	a,r7
	mov	r3,a
	clr	c
	mov	a,r4
	subb	a,r0
	mov	r4,a
	mov	a,r5
	subb	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#0x12E4
	mov	dpl,a
	mov	a,dph
	addc	a,#(0x12E4>>8)
	mov	dph,a
00009$:
	__endasm;
	__asm
	;; stage 2
	mov	a,r2
	rr	a
	rr	a
	anl	a,#0x3f
	mov	r0,a
	mov	a,r3
	rr	a
	rr	a
	mov	r1,a
	anl	a,#0xc0
	orl	a,r0
	mov	r0,a
	mov	a,r1
	anl	a,#0x3f
	mov	r1,a
	mov	a,r3
	rlc	a
	subb	a,acc
	anl	a,#0xc0
	orl	a,r1
	mov	r1,a
	mov	a,r4
	rr	a
	rr	a
	anl	a,#0x3f
	mov	r6,a
	mov	a,r5
	rr	a
	rr	a
	mov	r7,a
	anl	a,#0xc0
	orl	a,r6
	mov	r6,a
	mov	a,r7
	anl	a,#0x3f
	mov	r7,a
	mov	a,r5
	rlc	a
	subb	a,acc
	anl	a,#0xc0
	orl	a,r7
	mov	r7,a
	mov	a,dph
	jb	acc.7,00012$
	clr	c
	mov	a,r2
	subb	a,r6
	mov	r2,a
	mov	a,r3
	subb	a,r7
	mov	r3,a
	mov	a,r4
	add	a,r0
	mov	r4,a
	mov	a,r5
	addc	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#-0x09FB
	mov	dpl,a
	mov	a,dph
	addc	a,#(-0x09FB)>>8
	mov	dph,a
	sjmp	00013$
00012$:	mov	a,r2
	add	a,r6
	mov	r2,a
	mov	a,r3
	addc	a,r7
	mov	r3,a
	clr	c
	mov	a,r4
	subb	a,r0
	mov	r4,a
	mov	a,r5
	subb	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#0x09FB
	mov	dpl,a
	mov	a,dph
	addc	a,#(0x09FB>>8)
	mov	dph,a
00013$:
	__endasm;
	__asm
	;; stage 3
	mov	a,r2
	swap	a
	rl	a
	anl	a,#0x1f
	mov	r0,a
	mov	a,r3
	swap	a
	rl	a
	mov	r1,a
	anl	a,#0xe0
	orl	a,r0
	mov	r0,a
	mov	a,r1
	anl	a,#0x1f
	mov	r1,a
	mov	a,r3
	rlc	a
	subb	a,acc
	anl	a,#0xe0
	orl	a,r1
	mov	r1,a
	mov	a,r4
	swap	a
	rl	a
	anl	a,#0x1f
	mov	r6,a
	mov	a,r5
	swap	a
	rl	a
	mov	r7,a
	anl	a,#0xe0
	orl	a,r6
	mov	r6,a
	mov	a,r7
	anl	a,#0x1f
	mov	r7,a
	mov	a,r5
	rlc	a
	subb	a,acc
	anl	a,#0xe0
	orl	a,r7
	mov	r7,a
	mov	a,dph
	jb	acc.7,00016$
	clr	c
	mov	a,r2
	subb	a,r6
	mov	r2,a
	mov	a,r3
	subb	a,r7
	mov	r3,a
	mov	a,r4
	add	a,r0
	mov	r4,a
	mov	a,r5
	addc	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#-0x0511
	mov	dpl,a
	mov	a,dph
	addc	a,#(-0x0511)>>8
	mov	dph,a
	sjmp	00017$
00016$:	mov	a,r2
	add	a,r6
	mov	r2,a
	mov	a,r3
	addc	a,r7
	mov	r3,a
	clr	c
	mov	a,r4
	subb	a,r0
	mov	r4,a
	mov	a,r5
	subb	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#0x0511
	mov	dpl,a
	mov	a,dph
	addc	a,#(0x0511>>8)
	mov	dph,a
00017$:
	__endasm;
	__asm
	;; stage 4
	mov	a,r2
	swap	a
	anl	a,#0x0f
	mov	r0,a
	mov	a,r3
	swap	a
	mov	r1,a
	anl	a,#0xf0
	orl	a,r0
	mov	r0,a
	mov	a,r1
	anl	a,#0x0f
	mov	r1,a
	mov	a,r3
	rlc	a
	subb	a,acc
	anl	a,#0xf0
	orl	a,r1
	mov	r1,a
	mov	a,r4
	swap	a
	anl	a,#0x0f
	mov	r6,a
	mov	a,r5
	swap	a
	mov	r7,a
	anl	a,#0xf0
	orl	a,r6
	mov	r6,a
	mov	a,r7
	anl	a,#0x0f
	mov	r7,a
	mov	a,r5
	rlc	a
	subb	a,acc
	anl	a,#0xf0
	orl	a,r7
	mov	r7,a
	mov	a,dph
	jb	acc.7,00020$
	clr	c
	mov	a,r2
	subb	a,r6
	mov	r2,a
	mov	a,r3
	subb	a,r7
	mov	r3,a
	mov	a,r4
	add	a,r0
	mov	r4,a
	mov	a,r5
	addc	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#-0x028B
	mov	dpl,a
	mov	a,dph
	addc	a,#(-0x028B)>>8
	mov	dph,a
	sjmp	00021$
00020$:	mov	a,r2
	add	a,r6
	mov	r2,a
	mov	a,r3
	addc	a,r7
	mov	r3,a
	clr	c
	mov	a,r4
	subb	a,r0
	mov	r4,a
	mov	a,r5
	subb	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#0x028B
	mov	dpl,a
	mov	a,dph
	addc	a,#(0x028B>>8)
	mov	dph,a
00021$:
	__endasm;
	__asm
	;; stage 5
	mov	a,r2
	swap	a
	rr	a
	anl	a,#0x07
	mov	r0,a
	mov	a,r3
	swap	a
	rr	a
	mov	r1,a
	anl	a,#0xf8
	orl	a,r0
	mov	r0,a
	mov	a,r1
	anl	a,#0x07
	mov	r1,a
	mov	a,r3
	rlc	a
	subb	a,acc
	anl	a,#0xf8
	orl	a,r1
	mov	r1,a
	mov	a,r4
	swap	a
	rr	a
	anl	a,#0x07
	mov	r6,a
	mov	a,r5
	swap	a
	rr	a
	mov	r7,a
	anl	a,#0xf8
	orl	a,r6
	mov	r6,a
	mov	a,r7
	anl	a,#0x07
	mov	r7,a
	mov	a,r5
	rlc	a
	subb	a,acc
	anl	a,#0xf8
	orl	a,r7
	mov	r7,a
	mov	a,dph
	jb	acc.7,00024$
	clr	c
	mov	a,r2
	subb	a,r6
	mov	r2,a
	mov	a,r3
	subb	a,r7
	mov	r3,a
	mov	a,r4
	add	a,r0
	mov	r4,a
	mov	a,r5
	addc	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#-0x0146
	mov	dpl,a
	mov	a,dph
	addc	a,#(-0x0146)>>8
	mov	dph,a
	sjmp	00025$
00024$:	mov	a,r2
	add	a,r6
	mov	r2,a
	mov	a,r3
	addc	a,r7
	mov	r3,a
	clr	c
	mov	a,r4
	subb	a,r0
	mov	r4,a
	mov	a,r5
	subb	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#0x0146
	mov	dpl,a
	mov	a,dph
	addc	a,#(0x0146>>8)
	mov	dph,a
00025$:
	__endasm;
	__asm
	;; stage 6
	mov	a,r2
	rl	a
	rl	a
	anl	a,#0x03
	mov	r0,a
	mov	a,r3
	rl	a
	rl	a
	mov	r1,a
	anl	a,#0xfc
	orl	a,r0
	mov	r0,a
	mov	a,r1
	anl	a,#0x03
	mov	r1,a
	mov	a,r3
	rlc	a
	subb	a,acc
	anl	a,#0xfc
	orl	a,r1
	mov	r1,a
	mov	a,r4
	rl	a
	rl	a
	anl	a,#0x03
	mov	r6,a
	mov	a,r5
	rl	a
	rl	a
	mov	r7,a
	anl	a,#0xfc
	orl	a,r6
	mov	r6,a
	mov	a,r7
	anl	a,#0x03
	mov	r7,a
	mov	a,r5
	rlc	a
	subb	a,acc
	anl	a,#0xfc
	orl	a,r7
	mov	r7,a
	mov	a,dph
	jb	acc.7,00028$
	clr	c
	mov	a,r2
	subb	a,r6
	mov	r2,a
	mov	a,r3
	subb	a,r7
	mov	r3,a
	mov	a,r4
	add	a,r0
	mov	r4,a
	mov	a,r5
	addc	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#-0x00A3
	mov	dpl,a
	mov	a,dph
	addc	a,#(-0x00A3)>>8
	mov	dph,a
	sjmp	00029$
00028$:	mov	a,r2
	add	a,r6
	mov	r2,a
	mov	a,r3
	addc	a,r7
	mov	r3,a
	clr	c
	mov	a,r4
	subb	a,r0
	mov	r4,a
	mov	a,r5
	subb	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#0x00A3
	mov	dpl,a
	mov	a,dph
	addc	a,#(0x00A3>>8)
	mov	dph,a
00029$:
	__endasm;
	__asm
	;; stage 7
	mov	a,r2
	rl	a
	anl	a,#0x01
	mov	r0,a
	mov	a,r3
	rl	a
	mov	r1,a
	anl	a,#0xfe
	orl	a,r0
	mov	r0,a
	mov	a,r1
	anl	a,#0x01
	mov	r1,a
	mov	a,r3
	rlc	a
	subb	a,acc
	anl	a,#0xfe
	orl	a,r1
	mov	r1,a
	mov	a,r4
	rl	a
	anl	a,#0x01
	mov	r6,a
	mov	a,r5
	rl	a
	mov	r7,a
	anl	a,#0xfe
	orl	a,r6
	mov	r6,a
	mov	a,r7
	anl	a,#0x01
	mov	r7,a
	mov	a,r5
	rlc	a
	subb	a,acc
	anl	a,#0xfe
	orl	a,r7
	mov	r7,a
	mov	a,dph
	jb	acc.7,00032$
	clr	c
	mov	a,r2
	subb	a,r6
	mov	r2,a
	mov	a,r3
	subb	a,r7
	mov	r3,a
	mov	a,r4
	add	a,r0
	mov	r4,a
	mov	a,r5
	addc	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#-0x0051
	mov	dpl,a
	mov	a,dph
	addc	a,#(-0x0051)>>8
	mov	dph,a
	sjmp	00033$
00032$:	mov	a,r2
	add	a,r6
	mov	r2,a
	mov	a,r3
	addc	a,r7
	mov	r3,a
	clr	c
	mov	a,r4
	subb	a,r0
	mov	r4,a
	mov	a,r5
	subb	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#0x0051
	mov	dpl,a
	mov	a,dph
	addc	a,#(0x0051>>8)
	mov	dph,a
00033$:
	__endasm;
	__asm
	;; stage 8
	mov	a,r3
	mov	r0,a
	rlc	a
	subb	a,acc
	mov	r1,a
	mov	a,r5
	mov	r6,a
	rlc	a
	subb	a,acc
	mov	r7,a
	mov	a,dph
	jb	acc.7,00036$
	clr	c
	mov	a,r2
	subb	a,r6
	mov	r2,a
	mov	a,r3
	subb	a,r7
	mov	r3,a
	mov	a,r4
	add	a,r0
	mov	r4,a
	mov	a,r5
	addc	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#-0x0029
	mov	dpl,a
	mov	a,dph
	addc	a,#(-0x0029)>>8
	mov	dph,a
	sjmp	00037$
00036$:	mov	a,r2
	add	a,r6
	mov	r2,a
	mov	a,r3
	addc	a,r7
	mov	r3,a
	clr	c
	mov	a,r4
	subb	a,r0
	mov	r4,a
	mov	a,r5
	subb	a,r1
	mov	r5,a
	mov	a,dpl
	add	a,#0x0029
	mov	dpl,a
	mov	a,dph
	addc	a,#(0x0029>>8)
	mov	dph,a
00037$:
	__endasm;
}

#endif
